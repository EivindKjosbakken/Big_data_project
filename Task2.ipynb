{"cells":[{"cell_type":"markdown","source":["#### Names of people in the group\n\nPlease write the names of the people in your group in the next cell."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83045409-70a5-4010-b1cf-9db8d98f2bad"}}},{"cell_type":"markdown","source":["Eivind Kjosbakken"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"059a269d-8f13-494e-84e5-9da5d873047b"}}},{"cell_type":"code","source":["# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n!pip install -q ipython_unittest"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52939e3d-35c7-4767-a59e-ba75f716e952"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Loading modules that we need\nimport unittest\nfrom pyspark.sql.dataframe import DataFrame\nfrom typing import Any"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"499e46f5-12a4-46de-a212-90f2e936461e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# A helper function to load a table (stored in Parquet format) from DBFS as a Spark DataFrame \ndef load_df(table_name: \"name of the table to load\") -> DataFrame:\n    return spark.read.parquet(table_name)\n\nusers_df = load_df(\"/user/hive/warehouse/users\")\ncomments_df = load_df(\"/user/hive/warehouse/comments\")\nposts_df = load_df(\"/user/hive/warehouse/posts\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f8c68e0-6bda-40a6-8588-381e51dc0a93"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 1: implenenting two helper functions\nImpelment these two functions:\n1. 'run_query' that gets a Spark SQL query and run it on df which is a Spark DataFrame; it returns the content of the first column of the first row of the DataFrame that is the output of the query;\n2. 'run_query2' that is similar to 'run_query' but instead of one DataFrame gets two; it returns the content of the first column of the first row of the DataFrame that is the output of the query.\n\nNote that the result of a Spark SQL query is itself a Spark DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a2274f0-1bd8-4812-83d2-f793587e9548"}}},{"cell_type":"code","source":["def run_query(query: \"a SQL query string\", df: \"the DataFrame that the query will be executed on\") -> Any:\n    ## YOUR IMPLEMENTATION ##\n    df.createOrReplaceTempView(\"df\") #so I can call the df when running a sql query\n    ans = spark.sql(query).first()[0] #running the query and only returning first column og first row with .first()[0]\n    return ans\n\ndef run_query2(query: \"a SQL query string\", df1: \"DataFrame A\", df2: \"DataFrame B\") -> Any:\n    ## YOUR IMPLEMENTATION ##\n    #same as for run_query over, but with two df's, making sure when using the query I call them \"df1\" and \"df2\" (in the queries i wrote further down)\n    df1.createOrReplaceTempView(\"df1\")\n    df2.createOrReplaceTempView(\"df2\")\n    ans = spark.sql(query).first()[0]\n    return ans\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5fce811-7dd2-4602-a243-18a36754c302"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 3: validating the implementations by running the tests\n\nRun the cell below and make sure that all the tests run successfully."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46400bac-c11c-4bee-81a8-67d42e3ca968"}}},{"cell_type":"code","source":["# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n%load_ext ipython_unittest"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a021cce-880b-42b1-a09d-6d2c2222124e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The ipython_unittest extension is already loaded. To reload it, use:\n  %reload_ext ipython_unittest\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The ipython_unittest extension is already loaded. To reload it, use:\n  %reload_ext ipython_unittest\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 2: writing a few queries\nWrite the following queries in SQL to be executed by Spark in the next cell.\n\n1. 'q1': find the 'Id' of the most recently created post ('df' is 'posts_df') \n2. 'q2': find the number users\n3. 'q3': find the 'Id' of the user who posted most number of answers\n4. 'q4': find the number of questions\n5. 'q5': find the display name of the user who posted most number of comments\n\nNote that 'q1' is already available below as an example. Moreover, remmebr that Spark supports ANSI SQL 2003 so your queries have to comply with that standard."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7de10aba-7e77-4baa-af35-2b4e37916071"}}},{"cell_type":"code","source":["q1 = \"SELECT * FROM df ORDER BY CreationDate DESC limit 1\"\n\n## YOUR IMPLEMENTATION ##\n#from \"df\" since that is what I called the dataframe in run_query(), count gives me the number of Id's (which are unique, and therefore gives me the number of users)\nq2 = \"SELECT COUNT(Id) FROM df\"\n\n## YOUR IMPLEMENTATION ##\n#posttypeid = 2 gives me the answer-posts, ordering descending so the highest number of posts is first, and then just taking the first element with limit 1\nq3 = \"SELECT OwnerUserId FROM df WHERE PostTypeId=2 GROUP BY OwnerUserId ORDER BY COUNT(Id) DESC limit 1 \"\n\n## YOUR IMPLEMENTATION ##\n#posttypeid=1 gives me the questions, and then counting each question by their unique identifier Id\nq4 = \"SELECT COUNT(Id) from df WHERE PostTypeId=1 limit 1\"\n\n## YOUR IMPLEMENTATION ##\n#joining df1 and df2 by the Id since it is unique, then grouping by displayname so I sum all each unique display names posts, then taking the one with highest number of comments, counting by #PostId #since it is a unique identifier for a comment\nq5 = \"select DisplayName from df1 inner join df2 on df1.Id=df2.UserId group by DisplayName order by count(PostId) DESC limit 1\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3273ef7-7f4d-4b5d-bcf7-9935c952e26d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%%unittest_main\nclass TestTask2(unittest.TestCase):\n   \n    def test_q1(self):\n        # find the id of the most recent post\n        r = run_query(q1, posts_df)\n        self.assertEqual(r, 95045)\n\n    def test_q2(self):\n        # find the number of the users\n        r = run_query(q2, users_df)\n        self.assertEqual(r, 91616)\n    \n    def test_q3(self):\n        # find the user id of the user who posted most number of answers\n        r = run_query(q3, posts_df)\n        self.assertEqual(r, 64377)\n    \n    def test_q4(self):\n        # find the number of questions\n        r = run_query(q4, posts_df)\n        self.assertEqual(r, 28950)\n\n    def test_q5(self):\n        # find the display name of the user who posted most number of comments\n        r = run_query2(q5, users_df, comments_df)\n        self.assertEqual(r, \"Neil Slater\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31ca3fb9-427c-425d-b334-0df9c208a21b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Success.....\n----------------------------------------------------------------------\nRan 5 tests in 5.387s\n\nOK\nOut[21]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Success.....\n----------------------------------------------------------------------\nRan 5 tests in 5.387s\n\nOK\nOut[21]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 4: answering to questions about Spark related concepts\n\nPlease answer the following questions. Write your answer in one to two short paragraphs. Don't copy-paste; instead, write your own understanding.\n\n1. What is the difference between 'DataFrame', 'Dataset', and 'Resilient Distributed Datasets (RDD)'? \n2. When do you suggest using RDDs instead of using DataFrames?\n3. What is the main benefit of using DataSets instead of DataFrames?\n\nWrite your answers in the next cell."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe8a2e0c-7e72-410b-b385-00503c0bc136"}}},{"cell_type":"markdown","source":["Your answers:\n\nWhat is the difference between 'DataFrame', 'Dataset', and 'Resilient Distributed Datasets (RDD)?\n\nFirst of all, RDD is not organized like dataframe is (dataframe is organized into columns). While RDD is just partitioned elements. Dataset is organized like DataFrame, but has some more functionality like better compile type safety. RDD and Dataset can work on both structured and unstructured data, while DataFrame require the data to be semi-structured or structured. Furthermore you can do different actions and transformations on each of them. Some operations can only be done on RDD, like the map() function. RDD has compile type safety, but lacks some optimization that the dataframe has (but df does not have compile type safety). Datasets have both, but at the same time it has limited with functions, so you might have to convert to dataframe when using them.\n\n\nWhen do you suggest using RDDs instead of using DataFrames?\n\nFirst of all, RDDs work on both structured and unstructured data, while dataframes requires the data to be atleast semistructured (since the data in organized in columns). So if you have data that is not structured, you should use a RDD. RDD should also be used if you want to do certain actions, and transformations on your data, since alot of actions and transformations can only be done on an RDD. For example the map function to manipulate every element in the RDD. Additionally RDDâ€™s are resilient, so in case of losing a partition of the RDD, the complete RDD could be reconstructed. This is what makes the RDD fault-tolerant, so if a node fails in a computation, you can reconstruct the RDD, and then continue on with the computation, without having to restart.\n\n\nWhat is the main benefit of using DataSets instead of DataFrames?\n\nCompile-time type safety, the ability to alter data without knowing how the data is structured (the schema of the data). This is the reason DataSet was created in the first place (because of the limitation of compile-time type safety of dataframes, it also gave better optimization). If you for example do something wrong with a variable type, you will be informed when the script is compiled, and not when its ran (so you do not get a runtime error)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c33ca21-1672-4c0c-a876-f73cbb8f65b2"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Task2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2457922486426844}},"nbformat":4,"nbformat_minor":0}
